{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from numpy import asarray, zeros\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Embedding, Dense, Dropout, CuDNNGRU, CuDNNLSTM, Bidirectional, Convolution1D, concatenate\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Flatten, Activation, SpatialDropout1D, BatchNormalization, MaxPool1D, MaxPooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(r'../'+filename, encoding='utf-8') as f:\n",
    "        data = f.read().split('\\n')\n",
    "        data = data[:len(data)-1] #Clear last null row\n",
    "        f.close()\n",
    "    data = [row for row in csv.reader(data, quotechar='\"', delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)]\n",
    "    data = pd.DataFrame(data[1:], columns = data[0]) #Transform to Pandas DataFrame\n",
    "    return data\n",
    "\n",
    "def preprocess(text):\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "    text= text.strip().lower().split(' ')\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def categorical_label(df):\n",
    "    df['EMPIRICAL'] = [1 if 'EMPIRICAL' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['ENGINEERING'] = [1 if 'ENGINEERING' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['THEORETICAL'] = [1 if 'THEORETICAL' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['OTHERS'] = [1 if 'OTHERS' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    \n",
    "def tokenize_title():\n",
    "    titles = pd.concat([train_df['Title'],test_df['Title']], ignore_index=True)\n",
    "    \n",
    "    #Using keras tokenizer to encode and decode \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(titles)\n",
    "    new_titles = [text_to_word_sequence(sen) for sen in titles] #new tokenizered titles\n",
    "    \n",
    "    #Texts to sequences\n",
    "    train_titles_encoded = t.texts_to_sequences(train_df['Title'])\n",
    "    test_titles_encoded = t.texts_to_sequences(test_df['Title'])\n",
    "    \n",
    "    #pad sequences\n",
    "    max_title_len = max([len(sen) for sen in new_titles]) #max length of abstract\n",
    "    train_titles_sequences = pad_sequences(train_titles_encoded, maxlen=max_title_len)\n",
    "    test_titles_sequences = pad_sequences(test_titles_encoded, maxlen=max_title_len)\n",
    "    \n",
    "    #get word index and vocab size\n",
    "    title_word_index = t.word_index\n",
    "    title_vocab_size = len(title_word_index)+1\n",
    "    \n",
    "    #Print overview\n",
    "    print('Title tokenized: \\n{}\\n'.format(new_titles[0]))\n",
    "    print('Train titles sequences feed to Embedding layer: \\n{}\\n'.format(train_titles_sequences[0]))\n",
    "    print('Max length of titles: {}\\n'.format(max_title_len))\n",
    "    print('Sample of word index: \\n{}\\n'.format(list(title_word_index.items())[:5]))\n",
    "    print('Vocabulary size: ', title_vocab_size)\n",
    "    \n",
    "    return new_titles, train_titles_sequences, test_titles_sequences, max_title_len, title_word_index, title_vocab_size\n",
    "\n",
    "def tokenize_abstract():\n",
    "    abstracts = pd.concat([train_df['Abstract'],test_df['Abstract']], ignore_index=True)\n",
    "    \n",
    "    #Using keras tokenizer to encode and decode \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(abstracts)\n",
    "    new_abstracts = [text_to_word_sequence(sen) for sen in abstracts] #new tokenizered abtracts\n",
    "    \n",
    "    #Texts to sequences\n",
    "    train_abstracts_encoded = t.texts_to_sequences(train_df['Abstract'])\n",
    "    test_abstracts_encoded = t.texts_to_sequences(test_df['Abstract'])\n",
    "    \n",
    "    #pad sequences\n",
    "    max_abstract_len = max([len(sen) for sen in new_abstracts]) #max length of abstract\n",
    "    train_abstracts_sequences = pad_sequences(train_abstracts_encoded, maxlen=max_abstract_len)\n",
    "    test_abstracts_sequences = pad_sequences(test_abstracts_encoded, maxlen=max_abstract_len)\n",
    "    \n",
    "    #get word index and vocab size\n",
    "    abstract_word_index = t.word_index\n",
    "    abstract_vocab_size = len(abstract_word_index)+1\n",
    "    \n",
    "    #Print overview\n",
    "    print('#############################################################\\n\\n')\n",
    "    print('Abstract tokenized: \\n{}\\n'.format(new_abstracts[0]))\n",
    "    print('Train Abstract sequences feed to Embedding layer: \\n{}\\n'.format(train_abstracts_sequences[0]))\n",
    "    print('Max length of abtracts: {}\\n'.format(max_abstract_len))\n",
    "    print('Sample of Abstract word index: \\n{}\\n'.format(list(abstract_word_index.items())[:5]))\n",
    "    print('Abstract vocabulary size: ', abstract_vocab_size)\n",
    "    \n",
    "    return new_abstracts, train_abstracts_sequences, test_abstracts_sequences, max_abstract_len, abstract_word_index, abstract_vocab_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #read file\n",
    "    train_df = read_file('task2_trainset.csv')\n",
    "    test_df = read_file('task2_public_testset.csv')\n",
    "    categorical_label(train_df) #Categorical label to multiple columns\n",
    "    \n",
    "    \"\"\"\n",
    "    #Use this if want to remove stopwords in Abstract\n",
    "    train_df['Abstract'] = train_df['Abstract'].apply(preprocess)\n",
    "    test_df['Abstract'] = test_df['Abstract'].apply(preprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    new_titles, train_titles_sequences,\\\n",
    "    test_titles_sequences, max_title_len,\\\n",
    "    title_word_index, title_vocab_size = tokenize_title()\n",
    "    \n",
    "    new_abstracts, train_abstracts_sequences,\\\n",
    "    test_abstracts_sequences, max_abstract_len,\\\n",
    "    abstract_word_index, abstract_vocab_size = tokenize_abstract()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#le = LabelEncoder()\n",
    "\n",
    "df_bert = pd.DataFrame({'id': train_df['Id'],\n",
    "                           'label': train_df['EMPIRICAL'],\n",
    "                           'alpha': ['a']*len(train_df),\n",
    "                           'text': train_df['Abstract'].replace('\\$', '', regex=True) \n",
    "                          })\n",
    " \n",
    "#df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    " \n",
    "# Creating test dataframe according to BERT\n",
    "df_test = test_df\n",
    "df_bert_test = pd.DataFrame({'order_id':df_test['Id'],\n",
    "                 'text':df_test['Abstract'].replace('\\$', '', regex=True)})\n",
    " \n",
    "# Saving dataframes to .tsv format as required by BERT\n",
    "df_bert.to_csv('data/train.tsv', sep='\\t', index=False, header=False)\n",
    "#df_bert_dev.to_csv('data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('data/test.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_theoretical = pd.read_csv('./output/bert_theoretical.tsv', sep='\\t', header=None)\n",
    "bert_engineering = pd.read_csv('./output/bert_engineering.tsv', sep='\\t', header=None)\n",
    "bert_empirical = pd.read_csv('./output/bert_empirical.tsv', sep='\\t', header=None)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'order_id': test_df['Id'],\n",
    "    'THEORETICAL': bert_theoretical.idxmax(axis=1),\n",
    "    'ENGINEERING': bert_engineering.idxmax(axis=1),\n",
    "    'EMPIRICAL': bert_empirical.idxmax(axis=1),\n",
    "    'OTHERS': [0]*test_df.shape[0]\n",
    "})\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test.loc[i, 'THEORETICAL'] == 0 and df_test.loc[i, 'ENGINEERING'] == 0 and df_test.loc[i, 'EMPIRICAL'] == 0:\n",
    "        df_test.loc[i, 'OTHERS'] == 1\n",
    "        \n",
    "df_test\n",
    "\n",
    "df_test.to_csv('submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10769"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['ENGINEERING'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
