{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 0\tlabel\tcritical/trivial\n",
    "# 1\ttitle\tstring\n",
    "# 2\treview\tstring\n",
    "# 3\tOverall rating\t1-5\n",
    "# 4\taspect rating-Value\t1-5\n",
    "# 5\taspect rating-Location\t1-5\n",
    "# 6\taspect rating-Rooms\t1-5\n",
    "# 7\taspect rating-Cleanliness\t1-5\n",
    "# 8\taspect rating-Service\t1-5\n",
    "# 9\taspect rating-Sleep Quality\t1-5\n",
    "# 10\taspect rating-Check in / front desk\t1-5\n",
    "# 11\taspect rating-Business service (e.g., internet access)\n",
    "# 12\ttravel_type\tno fill in : 0\n",
    "        # traveled solo：1\n",
    "        # traveled as a couple：2\n",
    "        # traveled with family：3\n",
    "        # traveled with friends：4\n",
    "        # traveled on business：5\n",
    "\n",
    "def get_hotelreview_dataframe(filepath):\n",
    "    with open(filepath,'r', encoding='UTF-8') as f :\n",
    "        label = []\n",
    "        title = []\n",
    "        content = []\n",
    "        rating =[]\n",
    "        aspect_value = []\n",
    "        aspect_location = []\n",
    "        aspect_rooms = []\n",
    "        aspect_cleaness = []\n",
    "        aspect_service = []\n",
    "        aspect_sleep = []\n",
    "        traveler_type = []\n",
    "        response_day = []\n",
    "        text = []\n",
    "        \n",
    "        for line in f.read().splitlines() :\n",
    "            items = line.split('\\t')\n",
    "            \n",
    "            label.append(items[0])\n",
    "            title.append(items[1])\n",
    "            content.append(items[2])\n",
    "            rating.append(items[3])\n",
    "            aspect_value.append(items[4])\n",
    "            aspect_location.append(items[5])\n",
    "            aspect_rooms.append(items[6])\n",
    "            aspect_cleaness.append(items[7])\n",
    "            aspect_service.append(items[8])\n",
    "            aspect_sleep.append(items[9])\n",
    "            traveler_type.append(items[10])\n",
    "            response_day.append(items[11])\n",
    "            text.append(items[1] + \" \" + items[2])\n",
    "            \n",
    "        hotel_review = {\n",
    "            \"label\" : label, \"title\" : title, \"content\" : content, \"rating\" : rating, \n",
    "            \"aspect_value\" : aspect_value,\n",
    "            \"aspect_location\" : aspect_location,\n",
    "            \"aspect_rooms\" : aspect_rooms,\n",
    "            \"aspect_cleaness\" : aspect_cleaness,\n",
    "            \"aspect_service\" : aspect_service,\n",
    "            \"aspect_sleep\" : aspect_sleep,\n",
    "            \"traveler_type\" : traveler_type,\n",
    "            \"text\" : text\n",
    "        }\n",
    "        return pd.DataFrame(hotel_review)\n",
    "    \n",
    "def preprocess_data(raw_df):\n",
    "    raw_df['label'] =  raw_df['label'].map({'critical' : 0, 'trivial' : 1}).astype(int)\n",
    "    #raw_df['traveler_type'] =  raw_df['traveler_type'].map({0 : 'no fill in', 1 : 'traveled with family', 2 : 'traveled with friends', 3 : 'traveled as a couple', 4 : 'traveled on business' , 5 : 'traveled solo'}).astype(int)\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9745\n",
      "   label                         title  \\\n",
      "0      0  Exceptional customer service   \n",
      "1      0                   Outstanding   \n",
      "2      1                          stay   \n",
      "3      0                  Weekend away   \n",
      "4      0    Convenient docklands hotel   \n",
      "\n",
      "                                             content rating aspect_value  \\\n",
      "0  This stay hotel rating anniversary stay perfec...      5            5   \n",
      "1  We stayed nights June room product excellent d...      3            3   \n",
      "2  Stayed June Sun 7th June concert o2 love moder...      5            4   \n",
      "3  Spent enjoyable weekend Hilton Canary Wharf st...      5            0   \n",
      "4  We stayed going concert O2 comfortable hotel e...      5            5   \n",
      "\n",
      "  aspect_location aspect_rooms aspect_cleaness aspect_service aspect_sleep  \\\n",
      "0               5            4               5              5            0   \n",
      "1               4            5               4              2            4   \n",
      "2               4            5               5              5            0   \n",
      "3               0            0               0              0            0   \n",
      "4               4            5               5              4            5   \n",
      "\n",
      "  traveler_type  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "df_training =  get_hotelreview_dataframe('review_train_0608.txt')\n",
    "df_test =  get_hotelreview_dataframe('review_test_0608.txt')\n",
    "\n",
    "cols = ['label', 'title', 'content', 'rating', 'aspect_value', 'aspect_location',  'aspect_rooms', 'aspect_cleaness',  'aspect_service', 'aspect_sleep', 'traveler_type'] #define df order\n",
    "print(len(df_training))\n",
    "df_training = df_training[cols]\n",
    "\n",
    "\n",
    "trainingData = preprocess_data(df_training)\n",
    "testData = preprocess_data(df_test)\n",
    "\n",
    "\n",
    "print(trainingData[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = dict()\n",
    "f = open('./glove.6B.300d.txt', encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2441\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e66dadec838c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mexpected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mx_train_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm_max_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional, BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "m_max_seq_len =30\n",
    "m_max_num_vocab = 20000\n",
    "tokenizer = Tokenizer(num_words = m_max_num_vocab, lower=True, split=\" \", char_level=False)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "predicted = []\n",
    "expected = []\n",
    "\n",
    "tokenizer.fit_on_texts(trainingData['text'])\n",
    "x_train_seq = tokenizer.texts_to_sequences(trainingData['text'])\n",
    "x_train = sequence.pad_sequences(x_train_seq, maxlen = m_max_seq_len)\n",
    "y_trainOneHot = np_utils.to_categorical(trainingData['label'])\n",
    "\n",
    "x_test_seq = tokenizer.texts_to_sequences(testData['text'])\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen = m_max_seq_len)\n",
    "y_testOneHot = np_utils.to_categorical(testData['label'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(m_max_num_vocab, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= m_max_num_vocab:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "main_input = Input(shape=(m_max_seq_len,), dtype='float64')\n",
    "embedder = Embedding(m_max_num_vocab, \n",
    "                    EMBEDDING_DIM, \n",
    "                    input_length=m_max_seq_len, \n",
    "                    weights=[embedding_matrix], \n",
    "                    trainable=False)\n",
    "\n",
    "\n",
    "embed = embedder(main_input)\n",
    "# cnn1，kernel_size = 3\n",
    "conv1_1 = Convolution1D(256, 3, padding='same')(embed)\n",
    "bn1_1 = BatchNormalization()(conv1_1)\n",
    "relu1_1 = Activation('relu')(bn1_1)\n",
    "drop1_1 = Dropout(0.35)(relu1_1 )\n",
    "conv1_2 = Convolution1D(128, 3, padding='same')(drop1_1)\n",
    "bn1_2 = BatchNormalization()(conv1_2)\n",
    "relu1_2 = Activation('relu')(bn1_2)\n",
    "drop1_2 = Dropout(0.35)(relu1_2 )\n",
    "cnn1 = MaxPool1D(pool_size=4)(drop1_2)\n",
    "\n",
    "# cnn2，kernel_size = 4\n",
    "conv2_1 = Convolution1D(256, 4, padding='same')(embed)\n",
    "bn2_1 = BatchNormalization()(conv2_1)\n",
    "relu2_1 = Activation('relu')(bn2_1)\n",
    "drop2_1 = Dropout(0.35)(relu2_1 )\n",
    "conv2_2 = Convolution1D(128, 4, padding='same')(drop2_1)\n",
    "bn2_2 = BatchNormalization()(conv2_2)\n",
    "relu2_2 = Activation('relu')(bn2_2)\n",
    "drop2_2 = Dropout(0.35)(relu2_2 )\n",
    "cnn2 = MaxPool1D(pool_size=4)(drop2_2)\n",
    "\n",
    "# cnn3，kernel_size = 5\n",
    "conv3_1 = Convolution1D(256, 5, padding='same')(embed)\n",
    "bn3_1 = BatchNormalization()(conv3_1)\n",
    "relu3_1 = Activation('relu')(bn3_1)\n",
    "drop3_1 = Dropout(0.35)(relu3_1 )\n",
    "conv3_2 = Convolution1D(128, 5, padding='same')(drop3_1)\n",
    "bn3_2 = BatchNormalization()(conv3_2)\n",
    "relu3_2 = Activation('relu')(bn3_2)\n",
    "drop3_2 = Dropout(0.35)(relu3_2 )\n",
    "cnn3 = MaxPool1D(pool_size=4)(drop3_2)\n",
    "\n",
    "# concatenate above 3 convolution layers\n",
    "cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "flat = Flatten()(cnn)\n",
    "drop = Dropout(0.5)(flat)\n",
    "fc = Dense(512)(drop)\n",
    "bn = BatchNormalization()(fc)\n",
    "main_output = Dense(2, activation='sigmoid')(bn)\n",
    "model = Model(inputs = main_input, outputs = main_output)\n",
    "\n",
    "\n",
    "\n",
    "main_input = Input(shape=(m_max_seq_len,), dtype='float64')\n",
    "embed = Embedding(m_max_num_vocab, \n",
    "                    EMBEDDING_DIM, \n",
    "                    input_length=m_max_seq_len, \n",
    "                    weights=[embedding_matrix], \n",
    "                    trainable=False)(main_input)\n",
    "\n",
    "cnn = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
    "cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(256)(cnn)\n",
    "rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
    "rnn = Dense(256)(rnn)\n",
    "con = concatenate([cnn,rnn], axis=-1)\n",
    "main_output = Dense(2, activation='softmax')(con)\n",
    "model = Model(inputs = main_input, outputs = main_output)\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "optmzr = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=optmzr, metrics = ['accuracy'])\n",
    "train_history = model.fit(x_train, y_trainOneHot, batch_size = 32, epochs = 10, verbose = 2, validation_data=(x_test, y_testOneHot))\n",
    "print('model training completed!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pre_probability = model.predict(x_test_review)\n",
    "prediction = pre_probability.argmax(axis=-1)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Classification report for classifier:\\n%s\\n\"\n",
    "    % ( metrics.classification_report(y_test_review, prediction)))\n",
    "\n",
    "import pandas_ml\n",
    "from pandas_ml import ConfusionMatrix\n",
    "confusion_matrix = ConfusionMatrix(y_test_review, prediction)\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy = ', accuracy_score(y_test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K  \n",
    "from keras.layers import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "  \n",
    "def dot_product(x, kernel):  \n",
    "    \"\"\" \n",
    "    Wrapper for dot product operation, in order to be compatible with both \n",
    "    Theano and Tensorflow \n",
    "    Args: \n",
    "        x (): input \n",
    "        kernel (): weights \n",
    "    Returns: \n",
    "    \"\"\"  \n",
    "    if K.backend() == 'tensorflow':  \n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)  \n",
    "    else:  \n",
    "        return K.dot(x, kernel)  \n",
    "  \n",
    "  \n",
    "class AttentionWithContext(Layer):  \n",
    "    \"\"\" \n",
    "    Attention operation, with a context/query vector, for temporal data. \n",
    "    Supports Masking. \n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] \n",
    "    \"Hierarchical Attention Networks for Document Classification\" \n",
    "    by using a context vector to assist the attention \n",
    "    # Input shape \n",
    "        3D tensor with shape: `(samples, steps, features)`. \n",
    "    # Output shape \n",
    "        2D tensor with shape: `(samples, features)`. \n",
    "    How to use: \n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. \n",
    "    The dimensions are inferred based on the output shape of the RNN. \n",
    "    Note: The layer has been tested with Keras 2.0.6 \n",
    "    Example: \n",
    "        model.add(LSTM(64, return_sequences=True)) \n",
    "        model.add(AttentionWithContext()) \n",
    "        # next add a Dense layer (for classification/regression) or whatever... \n",
    "    \"\"\"  \n",
    "  \n",
    "    def __init__(self,  \n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,  \n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,  \n",
    "                 bias=True, **kwargs):  \n",
    "  \n",
    "        self.supports_masking = True  \n",
    "        self.init = initializers.get('glorot_uniform')  \n",
    "  \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)  \n",
    "        self.u_regularizer = regularizers.get(u_regularizer)  \n",
    "        self.b_regularizer = regularizers.get(b_regularizer)  \n",
    "  \n",
    "        self.W_constraint = constraints.get(W_constraint)  \n",
    "        self.u_constraint = constraints.get(u_constraint)  \n",
    "        self.b_constraint = constraints.get(b_constraint)  \n",
    "  \n",
    "        self.bias = bias  \n",
    "        super(AttentionWithContext, self).__init__(**kwargs)  \n",
    "  \n",
    "    def build(self, input_shape):  \n",
    "        assert len(input_shape) == 3  \n",
    "  \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),  \n",
    "                                 initializer=self.init,  \n",
    "                                 name='{}_W'.format(self.name),  \n",
    "                                 regularizer=self.W_regularizer,  \n",
    "                                 constraint=self.W_constraint)  \n",
    "        if self.bias:  \n",
    "            self.b = self.add_weight((input_shape[-1],),  \n",
    "                                     initializer='zero',  \n",
    "                                     name='{}_b'.format(self.name),  \n",
    "                                     regularizer=self.b_regularizer,  \n",
    "                                     constraint=self.b_constraint)  \n",
    "  \n",
    "        self.u = self.add_weight((input_shape[-1],),  \n",
    "                                 initializer=self.init,  \n",
    "                                 name='{}_u'.format(self.name),  \n",
    "                                 regularizer=self.u_regularizer,  \n",
    "                                 constraint=self.u_constraint)  \n",
    "  \n",
    "        super(AttentionWithContext, self).build(input_shape)  \n",
    "  \n",
    "    def compute_mask(self, input, input_mask=None):  \n",
    "        # do not pass the mask to the next layers  \n",
    "        return None  \n",
    "  \n",
    "    def call(self, x, mask=None):  \n",
    "        uit = dot_product(x, self.W)  \n",
    "  \n",
    "        if self.bias:  \n",
    "            uit += self.b  \n",
    "  \n",
    "        uit = K.tanh(uit)  \n",
    "        ait = dot_product(uit, self.u)  \n",
    "  \n",
    "        a = K.exp(ait)  \n",
    "  \n",
    "        # apply mask after the exp. will be re-normalized next  \n",
    "        if mask is not None:  \n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano  \n",
    "            a *= K.cast(mask, K.floatx())  \n",
    "  \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero  \n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.  \n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())  \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())  \n",
    "  \n",
    "        a = K.expand_dims(a)  \n",
    "        weighted_input = x * a  \n",
    "        return K.sum(weighted_input, axis=1)  \n",
    "  \n",
    "    def compute_output_shape(self, input_shape):  \n",
    "        return input_shape[0], input_shape[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
