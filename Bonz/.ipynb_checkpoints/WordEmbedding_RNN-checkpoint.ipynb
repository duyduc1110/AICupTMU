{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= For International Students ======= \n",
    "Write a program to construct dictionary of corpus EnglishDataset_Assignment1.txt. (there are 25000 IMDB movie reviews) You have to do:\n",
    " - Preprocessing: tokenization, stopword removal, remove punctuation, and stemming (simple normalization if needed).\n",
    " - Sort terms by term frequency and draw a figure to prof they follow Zipfâ€˜s law (long-tail distribution). \n",
    " - Rank terms by global TF-IDF.\n",
    " - Save the result as a txt file. \n",
    "\n",
    "\n",
    "# Preprocessing Data\n",
    "\n",
    "## Import all needed packages\n",
    "Please install all packages first\n",
    "\n",
    "!!! Use `nltk.download()` to download all NLTK dataset to 'C:\\nltk_data' if you don't have\n",
    "\n",
    "Please refer to this link: https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task2_trainset.csv', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = data[:len(data)-1] #Clear last null row\n",
    "    f.close()\n",
    "    \n",
    "data = [row for row in csv.reader(data, quotechar='\"', delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)]\n",
    "data = pd.DataFrame(data[1:], columns = data[0]) #Transform to Pandas DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization by using NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_word(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def lowercase(words):\n",
    "    return str(words).lower()\n",
    "\n",
    "#tokenize abstract and title\n",
    "data['Title_tokenized'] = [tokenize_word(lowercase(data['Title'][i])) for i in range(len(data))]\n",
    "data['Abstract_tokenized'] = [tokenize_word(lowercase(data['Abstract'][i])) for i in range(len(data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "1. Remove punctuation and whitespace words\n",
    "2. Replace number\n",
    "3. Remove stop words\n",
    "4. Stemming\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_whitespace(words): #Remove all punctuation and whitespace characters\n",
    "    pun_and_ws = string.punctuation+string.whitespace\n",
    "    new_words = [''.join(c for c in w if c not in pun_and_ws) for w in words]\n",
    "    new_words = [w for w in new_words if w != '']\n",
    "    return new_words\n",
    "\n",
    "def replace_number(words): #Converse number to text. Eg: '1' to 'one'\n",
    "    p = inflect.engine()\n",
    "    new_words = [p.number_to_words(w) if w.isdigit() else w for w in words]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words): #Remove stopwords by using nltk.corpus.stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    new_words = [w for w in words if w not in stop_words]\n",
    "    return new_words\n",
    "\n",
    "# I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "\n",
    "def stem_words(words): #Porter seems to be better than Lancaster\n",
    "    stemmer = LancasterStemmer()\n",
    "    new_words = [stemmer.stem(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def normalize(data):   \n",
    "    #Remove punctuations and whitespaces\n",
    "    data = [remove_punctuation_whitespace(words) for words in data]\n",
    "    #data = [remove_stopwords(words) for words in data]\n",
    "    \"\"\"\n",
    "    words = [replace_number(w) for w in words]\n",
    "    \n",
    "    # words = [stem_words(w) for w in words] \n",
    "    # I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "    words = [lemmatize_verbs(w) for w in words]\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data['Title_tokenized'] = normalize(data['Title_tokenized'])\n",
    "    data['Abstract_tokenized'] = normalize(data['Abstract_tokenized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2019-10-27 22:45:38,712 : INFO : collecting all words and their counts\n",
      "2019-10-27 22:45:38,715 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-27 22:45:38,904 : INFO : collected 35641 word types from a corpus of 1076212 raw words and 7000 sentences\n",
      "2019-10-27 22:45:38,905 : INFO : Loading a fresh vocabulary\n",
      "2019-10-27 22:45:38,929 : INFO : min_count=5 retains 10404 unique words (29% of original 35641, drops 25237)\n",
      "2019-10-27 22:45:38,929 : INFO : min_count=5 leaves 1035311 word corpus (96% of original 1076212, drops 40901)\n",
      "2019-10-27 22:45:38,962 : INFO : deleting the raw counts dictionary of 35641 items\n",
      "2019-10-27 22:45:38,963 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2019-10-27 22:45:38,964 : INFO : downsampling leaves estimated 789566 word corpus (76.3% of prior 1035311)\n",
      "2019-10-27 22:45:38,989 : INFO : estimated required memory for 10404 words and 300 dimensions: 30171600 bytes\n",
      "2019-10-27 22:45:38,989 : INFO : resetting layer weights\n",
      "2019-10-27 22:45:39,133 : INFO : training model with 12 workers on 10404 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-27 22:45:39,635 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-27 22:45:39,639 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-27 22:45:39,641 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-27 22:45:39,646 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-27 22:45:39,649 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-27 22:45:39,652 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-27 22:45:39,655 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-27 22:45:39,658 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-27 22:45:39,659 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-27 22:45:39,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-27 22:45:39,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-27 22:45:39,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-27 22:45:39,673 : INFO : EPOCH - 1 : training on 1076212 raw words (789520 effective words) took 0.5s, 1482851 effective words/s\n",
      "2019-10-27 22:45:40,171 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-27 22:45:40,179 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-27 22:45:40,182 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-27 22:45:40,185 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-27 22:45:40,190 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-27 22:45:40,197 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-27 22:45:40,198 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-27 22:45:40,199 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-27 22:45:40,201 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-27 22:45:40,205 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-27 22:45:40,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-27 22:45:40,209 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-27 22:45:40,210 : INFO : EPOCH - 2 : training on 1076212 raw words (789696 effective words) took 0.5s, 1488352 effective words/s\n",
      "2019-10-27 22:45:40,704 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-27 22:45:40,707 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-27 22:45:40,712 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-27 22:45:40,714 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-27 22:45:40,721 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-27 22:45:40,724 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-27 22:45:40,726 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-27 22:45:40,729 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-27 22:45:40,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-27 22:45:40,734 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-27 22:45:40,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-27 22:45:40,742 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-27 22:45:40,742 : INFO : EPOCH - 3 : training on 1076212 raw words (789814 effective words) took 0.5s, 1502049 effective words/s\n",
      "2019-10-27 22:45:41,229 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-27 22:45:41,238 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-27 22:45:41,241 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-27 22:45:41,248 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-27 22:45:41,254 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-27 22:45:41,255 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-27 22:45:41,258 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-27 22:45:41,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-27 22:45:41,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-27 22:45:41,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-27 22:45:41,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-27 22:45:41,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-27 22:45:41,274 : INFO : EPOCH - 4 : training on 1076212 raw words (789537 effective words) took 0.5s, 1503984 effective words/s\n",
      "2019-10-27 22:45:41,772 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-27 22:45:41,775 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-27 22:45:41,776 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-27 22:45:41,782 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-27 22:45:41,787 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-27 22:45:41,789 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-27 22:45:41,793 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-27 22:45:41,797 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-27 22:45:41,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-27 22:45:41,803 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-27 22:45:41,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-27 22:45:41,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-27 22:45:41,809 : INFO : EPOCH - 5 : training on 1076212 raw words (789808 effective words) took 0.5s, 1494014 effective words/s\n",
      "2019-10-27 22:45:41,809 : INFO : training on a 5381060 raw words (3948375 effective words) took 2.7s, 1475988 effective words/s\n",
      "2019-10-27 22:45:41,810 : INFO : saving Word2Vec object under WE_CBOW.model, separately None\n",
      "2019-10-27 22:45:41,810 : INFO : not storing attribute vectors_norm\n",
      "2019-10-27 22:45:41,811 : INFO : not storing attribute cum_table\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-27 22:45:42,028 : INFO : saved WE_CBOW.model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(data['Abstract_tokenized'], size=300, workers=12)\n",
    "    model.save('WE_CBOW.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMPIRICAL',\n",
       " 'ENGINEERING',\n",
       " 'ENGINEERING EMPIRICAL',\n",
       " 'OTHERS',\n",
       " 'THEORETICAL',\n",
       " 'THEORETICAL EMPIRICAL',\n",
       " 'THEORETICAL ENGINEERING',\n",
       " 'THEORETICAL ENGINEERING EMPIRICAL'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['Task 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'THEORETICAL' in data['Task 2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EMPIRICAL'] = [1 if 'EMPIRICAL' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['ENGINEERING'] = [1 if 'ENGINEERING' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['THEORETICAL'] = [1 if 'THEORETICAL' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['OTHERS'] = [1 if 'OTHERS' in data['Task 2'][i] else 0 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing text dataset')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from string import punctuation, ascii_lowercase\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# replace urls\n",
    "re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n",
    "                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n",
    "                    re.MULTILINE|re.UNICODE)\n",
    "# replace ips\n",
    "re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n",
    "\n",
    "# setup tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def text_to_wordlist(text, lower=False):\n",
    "    # replace URLs\n",
    "    text = re_url.sub(\"URL\", text)\n",
    "    \n",
    "    # replace IPs\n",
    "    text = re_ip.sub(\"IPADDRESS\", text)\n",
    "    \n",
    "    # Tokenize\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # optional: lower case\n",
    "    if lower:\n",
    "        text = [t.lower() for t in text]\n",
    "    \n",
    "    # Return a list of words\n",
    "    vocab.update(text)\n",
    "    return text\n",
    "\n",
    "def process_comments(list_sentences, lower=False):\n",
    "    comments = []\n",
    "    for text in tqdm(list_sentences):\n",
    "        txt = text_to_wordlist(text, lower=lower)\n",
    "        comments.append(txt)\n",
    "    return comments\n",
    "\n",
    "\n",
    "list_sentences_train = list(train_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n",
    "list_sentences_test = list(test_df[\"comment_text\"].fillna(\"NAN_WORD\").values)\n",
    "\n",
    "comments = process_comments(list_sentences_train + list_sentences_test, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict_keys' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f84f4c6920b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m sequences = [[word_index.get(t, 0) for t in comment]\n\u001b[0;32m      8\u001b[0m              for comment in comments[:len(list_sentences_train)]]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict_keys' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "sequences = [[word_index.get(t, 0) for t in comment]\n",
    "             for comment in comments[:len(list_sentences_train)]]\n",
    "test_sequences = [[word_index.get(t, 0)  for t in comment] \n",
    "                  for comment in comments[len(list_sentences_train):]]\n",
    "\n",
    "# pad\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WV_DIM = 100\n",
    "nb_words = min(MAX_NB_WORDS, len(word_vectors.vocab))\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(nb_words, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP THE COMMENT CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "wv_layer = Embedding(nb_words,\n",
    "                     WV_DIM,\n",
    "                     mask_zero=False,\n",
    "                     weights=[wv_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False)\n",
    "\n",
    "# Inputs\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "# biGRU\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[comment_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit([data_new], y, validation_split=0.1,epochs=10, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "history = pd.DataFrame(hist.history)\n",
    "plt.figure(figsize=(12,12));\n",
    "plt.plot(history[\"loss\"]);\n",
    "plt.plot(history[\"val_loss\"]);\n",
    "plt.title(\"Loss with pretrained word vectors\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_layer = Embedding(nb_words,\n",
    "                     WV_DIM,\n",
    "                     mask_zero=False,\n",
    "                     # weights=[wv_matrix],\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False)\n",
    "\n",
    "# Inputs\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "# biGRU\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[comment_input], outputs=preds, metrics='accuracy')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([data_new], y, validation_split=0.1,\n",
    "                 epochs=10, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "plt.figure(figsize=(12,12));\n",
    "plt.plot(history[\"loss\"]);\n",
    "plt.plot(history[\"val_loss\"]);\n",
    "plt.title(\"Loss with random word vectors\");\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "print(a[:2])\n",
    "print(a[:-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
