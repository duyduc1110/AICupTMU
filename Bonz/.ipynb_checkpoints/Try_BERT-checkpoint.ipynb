{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize, WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from numpy import asarray, zeros\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Embedding, Dense, Dropout, CuDNNGRU, CuDNNLSTM, Bidirectional, Convolution1D, concatenate\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Flatten, Activation, SpatialDropout1D, BatchNormalization, MaxPool1D, MaxPooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(r'../'+filename, encoding='utf-8') as f:\n",
    "        data = f.read().split('\\n')\n",
    "        data = data[:len(data)-1] #Clear last null row\n",
    "        f.close()\n",
    "    data = [row for row in csv.reader(data, quotechar='\"', delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)]\n",
    "    data = pd.DataFrame(data[1:], columns = data[0]) #Transform to Pandas DataFrame\n",
    "    return data\n",
    "\n",
    "def preprocess(text):\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "    text= text.strip().lower().split(' ')\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "def categorical_label(df):\n",
    "    df['EMPIRICAL'] = [1 if 'EMPIRICAL' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['ENGINEERING'] = [1 if 'ENGINEERING' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['THEORETICAL'] = [1 if 'THEORETICAL' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    df['OTHERS'] = [1 if 'OTHERS' in df.loc[i, 'Task 2'] else 0 for i in range(len(df))]\n",
    "    \n",
    "def tokenize_title():\n",
    "    titles = pd.concat([train_df['Title'],test_df['Title']], ignore_index=True)\n",
    "    \n",
    "    #Using keras tokenizer to encode and decode \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(titles)\n",
    "    new_titles = [text_to_word_sequence(sen) for sen in titles] #new tokenizered titles\n",
    "    \n",
    "    #Texts to sequences\n",
    "    train_titles_encoded = t.texts_to_sequences(train_df['Title'])\n",
    "    test_titles_encoded = t.texts_to_sequences(test_df['Title'])\n",
    "    \n",
    "    #pad sequences\n",
    "    max_title_len = max([len(sen) for sen in new_titles]) #max length of abstract\n",
    "    train_titles_sequences = pad_sequences(train_titles_encoded, maxlen=max_title_len)\n",
    "    test_titles_sequences = pad_sequences(test_titles_encoded, maxlen=max_title_len)\n",
    "    \n",
    "    #get word index and vocab size\n",
    "    title_word_index = t.word_index\n",
    "    title_vocab_size = len(title_word_index)+1\n",
    "    \n",
    "    #Print overview\n",
    "    print('Title tokenized: \\n{}\\n'.format(new_titles[0]))\n",
    "    print('Train titles sequences feed to Embedding layer: \\n{}\\n'.format(train_titles_sequences[0]))\n",
    "    print('Max length of titles: {}\\n'.format(max_title_len))\n",
    "    print('Sample of word index: \\n{}\\n'.format(list(title_word_index.items())[:5]))\n",
    "    print('Vocabulary size: ', title_vocab_size)\n",
    "    \n",
    "    return new_titles, train_titles_sequences, test_titles_sequences, max_title_len, title_word_index, title_vocab_size\n",
    "\n",
    "def tokenize_abstract():\n",
    "    abstracts = pd.concat([train_df['Abstract'],test_df['Abstract']], ignore_index=True)\n",
    "    \n",
    "    #Using keras tokenizer to encode and decode \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(abstracts)\n",
    "    new_abstracts = [text_to_word_sequence(sen) for sen in abstracts] #new tokenizered abtracts\n",
    "    \n",
    "    #Texts to sequences\n",
    "    train_abstracts_encoded = t.texts_to_sequences(train_df['Abstract'])\n",
    "    test_abstracts_encoded = t.texts_to_sequences(test_df['Abstract'])\n",
    "    \n",
    "    #pad sequences\n",
    "    max_abstract_len = max([len(sen) for sen in new_abstracts]) #max length of abstract\n",
    "    train_abstracts_sequences = pad_sequences(train_abstracts_encoded, maxlen=max_abstract_len)\n",
    "    test_abstracts_sequences = pad_sequences(test_abstracts_encoded, maxlen=max_abstract_len)\n",
    "    \n",
    "    #get word index and vocab size\n",
    "    abstract_word_index = t.word_index\n",
    "    abstract_vocab_size = len(abstract_word_index)+1\n",
    "    \n",
    "    #Print overview\n",
    "    print('#############################################################\\n\\n')\n",
    "    print('Abstract tokenized: \\n{}\\n'.format(new_abstracts[0]))\n",
    "    print('Train Abstract sequences feed to Embedding layer: \\n{}\\n'.format(train_abstracts_sequences[0]))\n",
    "    print('Max length of abtracts: {}\\n'.format(max_abstract_len))\n",
    "    print('Sample of Abstract word index: \\n{}\\n'.format(list(abstract_word_index.items())[:5]))\n",
    "    print('Abstract vocabulary size: ', abstract_vocab_size)\n",
    "    \n",
    "    return new_abstracts, train_abstracts_sequences, test_abstracts_sequences, max_abstract_len, abstract_word_index, abstract_vocab_size\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #read file\n",
    "    train_df = read_file('task2_trainset.csv')\n",
    "    test_df = read_file('task2_public_testset.csv')\n",
    "    \n",
    "    \"\"\"\n",
    "    #Use this if want to remove stopwords in Abstract\n",
    "    train_df['Abstract'] = train_df['Abstract'].apply(preprocess)\n",
    "    test_df['Abstract'] = test_df['Abstract'].apply(preprocess)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    categorical_label(train_df) #Categorical label to multiple columns\n",
    "    \n",
    "    new_titles, train_titles_sequences,\\\n",
    "    test_titles_sequences, max_title_len,\\\n",
    "    title_word_index, title_vocab_size = tokenize_title()\n",
    "    \n",
    "    new_abstracts, train_abstracts_sequences,\\\n",
    "    test_abstracts_sequences, max_abstract_len,\\\n",
    "    abstract_word_index, abstract_vocab_size = tokenize_abstract()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c0f4276781a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Saving dataframes to .tsv format as required by BERT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf_bert_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/train.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mdf_bert_dev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/dev.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf_bert_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3227\u001b[0m         )\n\u001b[1;32m-> 3228\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             )\n\u001b[0;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.tsv'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_bert = pd.DataFrame({'id': train_df['Id'],\n",
    "                           'label': label_new,\n",
    "                           'alpha': ['a']*len(train_df),\n",
    "                           'text': train_df['Abstract'].replace('\\$', '', regex=True) \n",
    "                          })\n",
    " \n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    " \n",
    "# Creating test dataframe according to BERT\n",
    "df_test = test_df\n",
    "df_bert_test = pd.DataFrame({'User_ID':df_test['Id'],\n",
    "                 'text':df_test['Abstract'].replace('\\$', '', regex=True)})\n",
    " \n",
    "# Saving dataframes to .tsv format as required by BERT\n",
    "df_bert_train.to_csv('data/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv('data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('data/test.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EMPIRICAL'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(train_df['Task 2'])\n",
    "le.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ENGINEERING', 'ENGINEERING', 'THEORETICAL ENGINEERING', ...,\n",
       "       'THEORETICAL', 'ENGINEERING', 'THEORETICAL'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = pd.read_csv('test_results.tsv', sep='\\t', header=None)\n",
    "result = []\n",
    "for i in range(rs.shape[0]):\n",
    "    pos = 0\n",
    "    for j in range(1, rs.shape[1]):\n",
    "        if rs.loc[i,j] > rs.loc[i,pos]:\n",
    "            pos = j\n",
    "    result.append(pos)\n",
    "test_label = le.inverse_transform(result)\n",
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['THEORETICAL'] = [1 if 'THEORETICAL' in i else 0 for i in test_label]\n",
    "test_df['ENGINEERING'] = [1 if 'ENGINEERING' in i else 0 for i in test_label]\n",
    "test_df['EMPIRICAL'] = [1 if 'EMPIRICAL' in i else 0 for i in test_label]\n",
    "test_df['OTHERS'] = [1 if 'OTHERS' in i else 0 for i in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'order_id': test_df['Id'],\n",
    "    'THEORETICAL': test_df['THEORETICAL'],\n",
    "    'ENGINEERING': test_df['ENGINEERING'],\n",
    "    'EMPIRICAL': test_df['EMPIRICAL'],\n",
    "    'OTHERS': test_df['OTHERS']\n",
    "})\n",
    "df_test.to_csv('submission.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
