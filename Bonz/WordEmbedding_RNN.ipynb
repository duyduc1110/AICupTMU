{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= For International Students ======= \n",
    "Write a program to construct dictionary of corpus EnglishDataset_Assignment1.txt. (there are 25000 IMDB movie reviews) You have to do:\n",
    " - Preprocessing: tokenization, stopword removal, remove punctuation, and stemming (simple normalization if needed).\n",
    " - Sort terms by term frequency and draw a figure to prof they follow Zipfâ€˜s law (long-tail distribution). \n",
    " - Rank terms by global TF-IDF.\n",
    " - Save the result as a txt file. \n",
    "\n",
    "\n",
    "# Preprocessing Data\n",
    "\n",
    "## Import all needed packages\n",
    "Please install all packages first\n",
    "\n",
    "!!! Use `nltk.download()` to download all NLTK dataset to 'C:\\nltk_data' if you don't have\n",
    "\n",
    "Please refer to this link: https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task2_trainset.csv', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = data[:len(data)-1] #Clear last null row\n",
    "    f.close()\n",
    "    \n",
    "data = [row for row in csv.reader(data, quotechar='\"', delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)]\n",
    "data = pd.DataFrame(data[1:], columns = data[0]) #Transform to Pandas DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization by using NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_word(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def lowercase(words):\n",
    "    return str(words).lower()\n",
    "\n",
    "#tokenize abstract and title\n",
    "data['Title_tokenized'] = [tokenize_word(lowercase(data['Title'][i])) for i in range(len(data))]\n",
    "data['Abstract_tokenized'] = [tokenize_word(lowercase(data['Abstract'][i])) for i in range(len(data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "1. Remove punctuation and whitespace words\n",
    "2. Replace number\n",
    "3. Remove stop words\n",
    "4. Stemming\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_whitespace(words): #Remove all punctuation and whitespace characters\n",
    "    pun_and_ws = string.punctuation+string.whitespace\n",
    "    new_words = [''.join(c for c in w if c not in pun_and_ws) for w in words]\n",
    "    new_words = [w for w in new_words if w != '']\n",
    "    return new_words\n",
    "\n",
    "def replace_number(words): #Converse number to text. Eg: '1' to 'one'\n",
    "    p = inflect.engine()\n",
    "    new_words = [p.number_to_words(w) if w.isdigit() else w for w in words]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words): #Remove stopwords by using nltk.corpus.stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    new_words = [w for w in words if w not in stop_words]\n",
    "    return new_words\n",
    "\n",
    "# I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "\n",
    "def stem_words(words): #Porter seems to be better than Lancaster\n",
    "    stemmer = LancasterStemmer()\n",
    "    new_words = [stemmer.stem(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def normalize(data):   \n",
    "    #Remove punctuations and whitespaces\n",
    "    data = [remove_punctuation_whitespace(words) for words in data]\n",
    "    #data = [remove_stopwords(words) for words in data]\n",
    "    \"\"\"\n",
    "    words = [replace_number(w) for w in words]\n",
    "    \n",
    "    # words = [stem_words(w) for w in words] \n",
    "    # I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "    words = [lemmatize_verbs(w) for w in words]\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data['Title_tokenized'] = normalize(data['Title_tokenized'])\n",
    "    data['Abstract_tokenized'] = normalize(data['Abstract_tokenized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2019-10-25 14:54:46,071 : INFO : collecting all words and their counts\n",
      "2019-10-25 14:54:46,074 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-25 14:54:46,260 : INFO : collected 35641 word types from a corpus of 1076212 raw words and 7000 sentences\n",
      "2019-10-25 14:54:46,261 : INFO : Loading a fresh vocabulary\n",
      "2019-10-25 14:54:46,286 : INFO : min_count=5 retains 10404 unique words (29% of original 35641, drops 25237)\n",
      "2019-10-25 14:54:46,286 : INFO : min_count=5 leaves 1035311 word corpus (96% of original 1076212, drops 40901)\n",
      "2019-10-25 14:54:46,320 : INFO : deleting the raw counts dictionary of 35641 items\n",
      "2019-10-25 14:54:46,321 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2019-10-25 14:54:46,321 : INFO : downsampling leaves estimated 789566 word corpus (76.3% of prior 1035311)\n",
      "2019-10-25 14:54:46,347 : INFO : estimated required memory for 10404 words and 300 dimensions: 30171600 bytes\n",
      "2019-10-25 14:54:46,348 : INFO : resetting layer weights\n",
      "2019-10-25 14:54:46,490 : INFO : training model with 12 workers on 10404 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-25 14:54:47,065 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-25 14:54:47,080 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-25 14:54:47,081 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-25 14:54:47,082 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-25 14:54:47,082 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-25 14:54:47,083 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-25 14:54:47,084 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-25 14:54:47,096 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-25 14:54:47,100 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-25 14:54:47,104 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:54:47,114 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:54:47,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:54:47,118 : INFO : EPOCH - 1 : training on 1076212 raw words (789757 effective words) took 0.6s, 1271592 effective words/s\n",
      "2019-10-25 14:54:47,695 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-25 14:54:47,699 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-25 14:54:47,705 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-25 14:54:47,709 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-25 14:54:47,711 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-25 14:54:47,718 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-25 14:54:47,719 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-25 14:54:47,722 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-25 14:54:47,729 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-25 14:54:47,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:54:47,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:54:47,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:54:47,741 : INFO : EPOCH - 2 : training on 1076212 raw words (790030 effective words) took 0.6s, 1284842 effective words/s\n",
      "2019-10-25 14:54:48,251 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-25 14:54:48,253 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-25 14:54:48,257 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-25 14:54:48,259 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-25 14:54:48,270 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-25 14:54:48,272 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-25 14:54:48,272 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-25 14:54:48,273 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-25 14:54:48,277 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-25 14:54:48,285 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:54:48,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:54:48,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:54:48,288 : INFO : EPOCH - 3 : training on 1076212 raw words (789911 effective words) took 0.5s, 1462606 effective words/s\n",
      "2019-10-25 14:54:48,878 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-25 14:54:48,879 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-25 14:54:48,882 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-25 14:54:48,887 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-25 14:54:48,892 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-25 14:54:48,893 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-25 14:54:48,895 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-25 14:54:48,899 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-25 14:54:48,900 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-25 14:54:48,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:54:48,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:54:48,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:54:48,914 : INFO : EPOCH - 4 : training on 1076212 raw words (789962 effective words) took 0.6s, 1276159 effective words/s\n",
      "2019-10-25 14:54:49,433 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-10-25 14:54:49,439 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-10-25 14:54:49,440 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-10-25 14:54:49,441 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-10-25 14:54:49,456 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-10-25 14:54:49,457 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-10-25 14:54:49,458 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-10-25 14:54:49,466 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-10-25 14:54:49,467 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-10-25 14:54:49,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-25 14:54:49,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-25 14:54:49,476 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-25 14:54:49,476 : INFO : EPOCH - 5 : training on 1076212 raw words (789325 effective words) took 0.6s, 1418428 effective words/s\n",
      "2019-10-25 14:54:49,477 : INFO : training on a 5381060 raw words (3948985 effective words) took 3.0s, 1322704 effective words/s\n",
      "2019-10-25 14:54:49,477 : INFO : saving Word2Vec object under WE_CBOW.model, separately None\n",
      "2019-10-25 14:54:49,478 : INFO : not storing attribute vectors_norm\n",
      "2019-10-25 14:54:49,479 : INFO : not storing attribute cum_table\n",
      "C:\\Users\\MrBonBon\\Miniconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-25 14:54:49,745 : INFO : saved WE_CBOW.model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = word2vec.Word2Vec(data['Abstract_tokenized'], size=300, workers=12)\n",
    "    model.save('WE_CBOW.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMPIRICAL',\n",
       " 'ENGINEERING',\n",
       " 'ENGINEERING EMPIRICAL',\n",
       " 'OTHERS',\n",
       " 'THEORETICAL',\n",
       " 'THEORETICAL EMPIRICAL',\n",
       " 'THEORETICAL ENGINEERING',\n",
       " 'THEORETICAL ENGINEERING EMPIRICAL'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['Task 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'THEORETICAL' in data['Task 2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['EMPIRICAL'] = [1 if 'EMPIRICAL' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['ENGINEERING'] = [1 if 'ENGINEERING' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['THEORETICAL'] = [1 if 'THEORETICAL' in data['Task 2'][i] else 0 for i in range(len(data))]\n",
    "data['OTHERS'] = [1 if 'OTHERS' in data['Task 2'][i] else 0 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "vocab = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "sequences = [[word_index.get(t, 0) for t in comment]\n",
    "             for comment in comments[:len(list_sentences_train)]]\n",
    "test_sequences = [[word_index.get(t, 0)  for t in comment] \n",
    "                  for comment in comments[len(list_sentences_train):]]\n",
    "\n",
    "# pad\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
