{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= For International Students ======= \n",
    "Write a program to construct dictionary of corpus EnglishDataset_Assignment1.txt. (there are 25000 IMDB movie reviews) You have to do:\n",
    " - Preprocessing: tokenization, stopword removal, remove punctuation, and stemming (simple normalization if needed).\n",
    " - Sort terms by term frequency and draw a figure to prof they follow Zipfâ€˜s law (long-tail distribution). \n",
    " - Rank terms by global TF-IDF.\n",
    " - Save the result as a txt file. \n",
    "\n",
    "\n",
    "# Preprocessing Data\n",
    "\n",
    "## Import all needed packages\n",
    "Please install all packages first\n",
    "\n",
    "!!! Use `nltk.download()` to download all NLTK dataset to 'C:\\nltk_data' if you don't have\n",
    "\n",
    "Please refer to this link: https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task2_trainset.csv', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = data[:len(data)-1] #Clear last null row\n",
    "    f.close()\n",
    "    \n",
    "data = [row for row in csv.reader(data, quotechar='\"', delimiter=',',quoting=csv.QUOTE_ALL, skipinitialspace=True)]\n",
    "data = pd.DataFrame(data[1:], columns = data[0]) #Transform to Pandas DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization by using NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_word(sentence):\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "def lowercase(words):\n",
    "    return str(words).lower()\n",
    "\n",
    "#tokenize abstract and title\n",
    "data['Title_tokenized'] = [tokenize_word(lowercase(data['Title'][i])) for i in range(len(data))]\n",
    "data['Abstract_tokenized'] = [tokenize_word(lowercase(data['Abstract'][i])) for i in range(len(data))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "1. Remove punctuation and whitespace words\n",
    "2. Replace number\n",
    "3. Remove stop words\n",
    "4. Stemming\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_whitespace(words): #Remove all punctuation and whitespace characters\n",
    "    pun_and_ws = string.punctuation+string.whitespace\n",
    "    new_words = [''.join(c for c in w if c not in pun_and_ws) for w in words]\n",
    "    new_words = [w for w in new_words if w != '']\n",
    "    return new_words\n",
    "\n",
    "def replace_number(words): #Converse number to text. Eg: '1' to 'one'\n",
    "    p = inflect.engine()\n",
    "    new_words = [p.number_to_words(w) if w.isdigit() else w for w in words]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words): #Remove stopwords by using nltk.corpus.stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    new_words = [w for w in words if w not in stop_words]\n",
    "    return new_words\n",
    "\n",
    "# I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "\n",
    "def stem_words(words): #Porter seems to be better than Lancaster\n",
    "    stemmer = LancasterStemmer()\n",
    "    new_words = [stemmer.stem(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return new_words\n",
    "\n",
    "def normalize(data):   \n",
    "    #Remove punctuations and whitespaces\n",
    "    data = [remove_punctuation_whitespace(words) for words in data]\n",
    "    data = [remove_stopwords(words) for words in data]\n",
    "    \"\"\"\n",
    "    words = [replace_number(w) for w in words]\n",
    "    \n",
    "    # words = [stem_words(w) for w in words] \n",
    "    # I decided to ignore stemming step because it caused a lot of bugs like: 'comedy' -> 'comedi'\n",
    "    words = [lemmatize_verbs(w) for w in words]\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data['Title_tokenized'] = normalize(data['Title_tokenized'])\n",
    "    data['Abstract_tokenized'] = normalize(data['Abstract_tokenized'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(data):\n",
    "    data = [list_to_string(d) for d in data]\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=True, analyzer='word', stop_words='english', max_df=0.9)\n",
    "    vectors = vectorizer.fit_transform(data)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense().tolist()\n",
    "    data_vectors = pd.DataFrame(dense, columns=feature_names)\n",
    "    return data_vectors\n",
    "\n",
    "def list_to_string(words):\n",
    "    return ' '.join(w for w in words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    abstract_vectorize = tfidf_vectorize(data['Abstract_tokenized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation\n",
    "1. Get a dic of word frequency. \n",
    "> Since there is the word 'would' in top 10, nltk.stopwords doesn't seem good\n",
    "2. Plot a line chart, it's look like Zipf's law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_freqdist(words):\n",
    "    new_words = [small_w for w in words for small_w in w]\n",
    "    return FreqDist(new_words)\n",
    "\n",
    "def print_sorted_tf(tf, number):\n",
    "    sorted_frequency = sorted(tf.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    print('Top {} high frequency words: \\n{}\\n'.format(number, sorted_frequency[0:number]))\n",
    "    \n",
    "def print_plot_freqdist(tf, number):\n",
    "    x = range(number)\n",
    "    y = sorted(list(tf.values()), reverse=True)[:number]\n",
    "    print('Frequency Distribution: \\n', plt.plot(x,y))\n",
    "    \n",
    "def get_tfidf(tf, words):\n",
    "    tfidf = dict([(w,0) for w in tf.keys()])\n",
    "    #Calculate df\n",
    "    for w in words:\n",
    "        distinct_w = set(w)\n",
    "        for key in distinct_w:\n",
    "            tfidf[key]+=1\n",
    "    #Calculate idf & tfidf\n",
    "    for key in tfidf:\n",
    "        tfidf[key] = math.log10(len(words)/tfidf[key])\n",
    "        tfidf[key] *= tf[key]\n",
    "    return tfidf\n",
    "\n",
    "def print_sorted_tfidf(tfidf, number):\n",
    "    sorted_tfidf = sorted(tfidf.items(), key = operator.itemgetter(1), reverse=True)\n",
    "    print('Top {} high TF-IDF words: \\n{}\\n'.format(number, sorted_tfidf[0:number]))\n",
    "    return sorted_tfidf\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    tf = nltk_freqdist(words) #I found this shorter way with nltk \n",
    "    print_sorted_tf(tf, 100)\n",
    "    tfidf = get_tfidf(tf, words)\n",
    "    sorted_tfidf = print_sorted_tfidf(tfidf, 100)\n",
    "    print_plot_freqdist(tf, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "data.to_csv('trainingdata_tokenized.csv', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
